doctype html
html(xmlns:og="http://opengraphprotocol.org/schema/" xmlns:fb="http://www.facebook.com/2008/fbml" xmlns:website="http://ogp.me/ns/website" lang="en-US" itemscope="" itemtype="http://schema.org/WebPage")
  head
    meta(http-equiv="X-UA-Compatible" content="IE=edge,chrome=1")
    meta(name="viewport" content="initial-scale=1")
    // This is Squarespace.
    // xiao-ling-wtaw
    base(href="")
    meta(charset="utf-8")
    title
      | Deep Reinforcement Learning with Hierarchical Recurrent Encoder-Decoder for Conversation — Xiao Ling
    link(rel="shortcut icon" type="image/x-icon" href="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/59c1643c80bd5ed79fe5faa1/favicon.ico")
    link(rel="canonical" href="https://lingxiaoling.me/drl-dialogue/")
    meta(property="og:site_name" content="Xiao Ling")
    meta(property="og:title" content="Deep Reinforcement Learning with Hierarchical Recurrent Encoder-Decoder for Conversation")
    meta(property="og:url" content="https://lingxiaoling.me/drl-dialogue/")
    meta(property="og:type" content="website")
    meta(property="og:image" content="http://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/59c164c9b078699a73cff3d5/1505846473671/HI.jpg?format=1000w")
    meta(property="og:image:width" content="750")
    meta(property="og:image:height" content="744")
    meta(itemprop="name" content="Deep Reinforcement Learning with Hierarchical Recurrent Encoder-Decoder for Conversation")
    meta(itemprop="url" content="https://lingxiaoling.me/drl-dialogue/")
    meta(itemprop="thumbnailUrl" content="http://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/59c164c9b078699a73cff3d5/1505846473671/HI.jpg?format=1000w")
    link(rel="image_src" href="http://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/59c164c9b078699a73cff3d5/1505846473671/HI.jpg?format=1000w")
    meta(itemprop="image" content="http://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/59c164c9b078699a73cff3d5/1505846473671/HI.jpg?format=1000w")
    meta(name="twitter:title" content="Deep Reinforcement Learning with Hierarchical Recurrent Encoder-Decoder for Conversation")
    meta(name="twitter:image" content="http://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/59c164c9b078699a73cff3d5/1505846473671/HI.jpg?format=1000w")
    meta(name="twitter:url" content="https://lingxiaoling.me/drl-dialogue/")
    meta(name="twitter:card" content="summary")
    meta(name="description" content="")
    script(type="text/javascript" src="//use.typekit.net/ik/9TQC0TqurNlOyzXBtNZ9fGm7S43mibgtFO3CRlejRfIfezvgfFHN4UJLFRbh52jhWD9ojcZcZRMUFQByFQ6X5AsRZRiKw2FRZs7bMPG0pe8ydkuC-Ao1OWiXjWS0SaBujW48Sagyjh90jhNlOeUTdcit-ABzde80ZkolZPUCdhFydeyzSabCiaiaOcUTdcit-ABzde80ZkolZPUaiaS0pe8ydkuC-Ao1OWiXjWS0SaBujW48Sagyjh90jhNlOYyTjAvlde80ZkuaieBaO1FUiABkZWF3jAF8OcFzdPJPjAszjc9lZhBkjAuzdcblSY4zH6qJN2bbMg6eJMJ7fbR3FgMMeMS6MKG4fV9XIMMjIfMfH6qJvQbbMg6sJMHbMj0LGwtB.js")
    script(type="text/javascript").
      try{Typekit.load();}catch(e){}
    link(rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Lato:900,700,700i,900i,400")
    script(type="text/javascript").
      SQUARESPACE_ROLLUPS = {};
    script.
      (function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//static.squarespace.com/universal/scripts-compressed/common-a7aaecd970d62f77123d-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-common');
    script(crossorigin="anonymous" src="//static.squarespace.com/universal/scripts-compressed/common-a7aaecd970d62f77123d-min.en-US.js")
    script(data-name="static-context").
      Static = window.Static || {}; Static.SQUARESPACE_CONTEXT = {"facebookAppId":"314192535267336","rollups":{"squarespace-announcement-bar":{"css":"//static.squarespace.com/universal/styles-compressed/announcement-bar-d41d8cd98f00b204e9800998ecf8427e-min.css","js":"//static.squarespace.com/universal/scripts-compressed/announcement-bar-b67a866831fce5a58824-min.en-US.js"},"squarespace-audio-player":{"css":"//static.squarespace.com/universal/styles-compressed/audio-player-6c42d60d26f4e09ab1ac335b4bc55b7e-min.css","js":"//static.squarespace.com/universal/scripts-compressed/audio-player-9a658d7298d5fd0da041-min.en-US.js"},"squarespace-blog-collection-list":{"css":"//static.squarespace.com/universal/styles-compressed/blog-collection-list-d41d8cd98f00b204e9800998ecf8427e-min.css","js":"//static.squarespace.com/universal/scripts-compressed/blog-collection-list-d0de40c544614fff6825-min.en-US.js"},"squarespace-calendar-block-renderer":{"css":"//static.squarespace.com/universal/styles-compressed/calendar-block-renderer-56713b1663346e1fe5b31a3adb8f31fc-min.css","js":"//static.squarespace.com/universal/scripts-compressed/calendar-block-renderer-cff5f27163ef88625272-min.en-US.js"},"squarespace-chartjs-helpers":{"css":"//static.squarespace.com/universal/styles-compressed/chartjs-helpers-9935a41d63cf08ca108505d288c1712e-min.css","js":"//static.squarespace.com/universal/scripts-compressed/chartjs-helpers-ade99c5272ecd0612cb6-min.en-US.js"},"squarespace-comments":{"css":"//static.squarespace.com/universal/styles-compressed/comments-c32adaffffa61675d5f238e2ea399b2b-min.css","js":"//static.squarespace.com/universal/scripts-compressed/comments-6b272b9bb4b56e3c557a-min.en-US.js"},"squarespace-commerce-cart":{"js":"//static.squarespace.com/universal/scripts-compressed/commerce-cart-c9eb49ffb559f6a891b8-min.en-US.js"},"squarespace-dialog":{"css":"//static.squarespace.com/universal/styles-compressed/dialog-4339295b394626bc805a3dff24f640fe-min.css","js":"//static.squarespace.com/universal/scripts-compressed/dialog-6b46e9f4914a208b827b-min.en-US.js"},"squarespace-events-collection":{"css":"//static.squarespace.com/universal/styles-compressed/events-collection-56713b1663346e1fe5b31a3adb8f31fc-min.css","js":"//static.squarespace.com/universal/scripts-compressed/events-collection-a4089937dbc331391991-min.en-US.js"},"squarespace-form-rendering-utils":{"js":"//static.squarespace.com/universal/scripts-compressed/form-rendering-utils-46fcaad2bd1479c9a15a-min.en-US.js"},"squarespace-forms":{"css":"//static.squarespace.com/universal/styles-compressed/forms-4af1b5b3c288ba939ea861fe2d37119a-min.css","js":"//static.squarespace.com/universal/scripts-compressed/forms-a805f22f077dc759befa-min.en-US.js"},"squarespace-gallery-collection-list":{"css":"//static.squarespace.com/universal/styles-compressed/gallery-collection-list-d41d8cd98f00b204e9800998ecf8427e-min.css","js":"//static.squarespace.com/universal/scripts-compressed/gallery-collection-list-59e2eb51d178d202dc8b-min.en-US.js"},"squarespace-image-zoom":{"css":"//static.squarespace.com/universal/styles-compressed/image-zoom-ae974921915aeccaff8ad60c60e19c31-min.css","js":"//static.squarespace.com/universal/scripts-compressed/image-zoom-6eb85675f4e2a39ed0cf-min.en-US.js"},"squarespace-pinterest":{"css":"//static.squarespace.com/universal/styles-compressed/pinterest-d41d8cd98f00b204e9800998ecf8427e-min.css","js":"//static.squarespace.com/universal/scripts-compressed/pinterest-fac72a7a9f4a9779411d-min.en-US.js"},"squarespace-popup-overlay":{"css":"//static.squarespace.com/universal/styles-compressed/popup-overlay-6b891e5f689f032ce65af5855c067b4f-min.css","js":"//static.squarespace.com/universal/scripts-compressed/popup-overlay-a122bf1c895c536a71e4-min.en-US.js"},"squarespace-product-quick-view":{"css":"//static.squarespace.com/universal/styles-compressed/product-quick-view-9abaaa4dfff182aa8d4ccf3b6ffdbe8d-min.css","js":"//static.squarespace.com/universal/scripts-compressed/product-quick-view-5beb792060e55707ff5e-min.en-US.js"},"squarespace-products-collection-item-v2":{"css":"//static.squarespace.com/universal/styles-compressed/products-collection-item-v2-ae974921915aeccaff8ad60c60e19c31-min.css","js":"//static.squarespace.com/universal/scripts-compressed/products-collection-item-v2-0b4c7a19a473c4a6b664-min.en-US.js"},"squarespace-products-collection-list-v2":{"css":"//static.squarespace.com/universal/styles-compressed/products-collection-list-v2-ae974921915aeccaff8ad60c60e19c31-min.css","js":"//static.squarespace.com/universal/scripts-compressed/products-collection-list-v2-459c818a57e24e25c479-min.en-US.js"},"squarespace-search-page":{"css":"//static.squarespace.com/universal/styles-compressed/search-page-9c747eeaabe96dacfea4932a63336f54-min.css","js":"//static.squarespace.com/universal/scripts-compressed/search-page-fc925fe50eb204c0a76c-min.en-US.js"},"squarespace-search-preview":{"js":"//static.squarespace.com/universal/scripts-compressed/search-preview-77baf003e1a66fc93e7a-min.en-US.js"},"squarespace-share-buttons":{"js":"//static.squarespace.com/universal/scripts-compressed/share-buttons-5bf1994810707b516c2c-min.en-US.js"},"squarespace-simple-liking":{"css":"//static.squarespace.com/universal/styles-compressed/simple-liking-09fa291ec2800c97714f0d157fd0a6ca-min.css","js":"//static.squarespace.com/universal/scripts-compressed/simple-liking-cb94a4b275616d9f3454-min.en-US.js"},"squarespace-social-buttons":{"css":"//static.squarespace.com/universal/styles-compressed/social-buttons-7a696232d1cd101fd62b5f174f9ae6ff-min.css","js":"//static.squarespace.com/universal/scripts-compressed/social-buttons-5ff6ae2c73ae9001debe-min.en-US.js"},"squarespace-tourdates":{"css":"//static.squarespace.com/universal/styles-compressed/tourdates-d41d8cd98f00b204e9800998ecf8427e-min.css","js":"//static.squarespace.com/universal/scripts-compressed/tourdates-7c6f8257ad99b2a94e27-min.en-US.js"},"squarespace-website-overlays-manager":{"css":"//static.squarespace.com/universal/styles-compressed/website-overlays-manager-923311a5ffb05ecb734ddfc7d9be08bc-min.css","js":"//static.squarespace.com/universal/scripts-compressed/website-overlays-manager-baa345ac6fe2ea41c29c-min.en-US.js"}},"pageType":2,"website":{"id":"59bfff0151a5849dcf7b03ce","identifier":"xiao-ling-wtaw","websiteType":1,"contentModifiedOn":1518656524167,"cloneable":false,"siteStatus":{},"language":"en-US","timeZone":"America/New_York","machineTimeZoneOffset":-14400000,"timeZoneOffset":-14400000,"timeZoneAbbr":"EDT","siteTitle":"Xiao Ling","siteDescription":"<p>Personal website. Machine Learning. Computer Vision. Natural language processing. Figure Drawing.</p>","socialLogoImageId":"59c164c9b078699a73cff3d5","shareButtonOptions":{"5":true,"3":true,"8":true,"2":true,"7":true,"1":true,"6":true,"4":true},"socialLogoImageUrl":"//static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/59c164c9b078699a73cff3d5/1518656524167/","authenticUrl":"https://lingxiaoling.me","internalUrl":"https://xiao-ling-wtaw.squarespace.com","baseUrl":"https://lingxiaoling.me","primaryDomain":"lingxiaoling.me","sslSetting":3,"isHstsEnabled":false,"typekitId":"","statsMigrated":false,"imageMetadataProcessingEnabled":false,"screenshotId":"bb0775fa"},"websiteSettings":{"id":"59bfff0151a5849dcf7b03d1","websiteId":"59bfff0151a5849dcf7b03ce","subjects":[],"country":"US","state":"PA","simpleLikingEnabled":true,"mobileInfoBarSettings":{"isContactEmailEnabled":false,"isContactPhoneNumberEnabled":false,"isLocationEnabled":false,"isBusinessHoursEnabled":false},"commentLikesAllowed":true,"commentAnonAllowed":true,"commentThreaded":true,"commentApprovalRequired":false,"commentAvatarsOn":true,"commentSortType":2,"commentFlagThreshold":0,"commentFlagsAllowed":true,"commentEnableByDefault":true,"commentDisableAfterDaysDefault":0,"disqusShortname":"","commentsEnabled":false,"storeSettings":{"returnPolicy":null,"termsOfService":null,"privacyPolicy":null,"paymentSettings":{},"expressCheckout":false,"continueShoppingLinkUrl":"/","useLightCart":false,"showNoteField":false,"shippingCountryDefaultValue":"US","billToShippingDefaultValue":false,"showShippingPhoneNumber":true,"isShippingPhoneRequired":false,"showBillingPhoneNumber":true,"isBillingPhoneRequired":false,"multipleQuantityAllowedForServices":true,"currenciesSupported":["CHF","HKD","MXN","EUR","DKK","USD","CAD","MYR","NOK","THB","AUD","SGD","ILS","PLN","GBP","CZK","SEK","NZD","PHP","RUB"],"defaultCurrency":"USD","selectedCurrency":"USD","measurementStandard":1,"showCustomCheckoutForm":false,"enableMailingListOptInByDefault":false,"sameAsRetailLocation":false,"isLive":false},"useEscapeKeyToLogin":true,"ssBadgeType":1,"ssBadgePosition":4,"ssBadgeVisibility":1,"ssBadgeDevices":1,"pinterestOverlayOptions":{"mode":"disabled"},"ampEnabled":false},"cookieSettings":{"isRestrictiveCookiePolicyEnabled":false},"websiteCloneable":false,"collection":{"title":"Deep Reinforcement Learning with Hierarchical Recurrent Encoder-Decoder for Conversation","id":"59c2e758197aea9115b6457b","fullUrl":"/drl-dialogue/","type":10},"subscribed":false,"appDomain":"squarespace.com","templateTweakable":true,"tweakJSON":{"tweak-blog-list-columns":"4","tweak-blog-list-item-image-aspect-ratio-grid":"1:1 Square","tweak-blog-list-item-image-aspect-ratio-stacked":"1:1 Square","tweak-blog-list-item-image-show":"false","tweak-blog-list-spacing":"60px","tweak-blog-list-style":"Stacked","tweak-cart-link-display":"Hide","tweak-display-social-icons":"false","tweak-full-width-first-landscape":"true","tweak-gallery-gutter":"12","tweak-gallery-image-caption-indicator-position":"Bottom Left","tweak-gallery-style":"Stacked","tweak-gallery-title-position":"Center","tweak-header-element-spacing":"1","tweak-header-outer-padding":"4","tweak-hide-index-desc-on-hover":"true","tweak-index-desc-position":"Middle","tweak-index-inactive-on-load":"true","tweak-index-menu-desc-spacing":"3","tweak-index-menu-padding":"4","tweak-index-nav-position":"Bottom Center","tweak-index-slideshow-animation":"false","tweak-index-slideshow-delay":"2","tweak-index-slideshow-on":"false","tweak-index-slideshow-touch-on":"true","tweak-nav-position":"Right","tweak-nav-style":"Standard","tweak-page-title-position":"Center","tweak-product-item-image-zoom-factor":"2.5","tweak-product-list-item-hover-behavior":"Fade","tweak-product-list-items-per-row":"3","tweak-show-gallery-image-captions":"Always","tweak-show-gallery-title-overlay":"In Index","tweak-show-index-desc":"true","tweak-show-page-title-overlay":"Never","tweak-site-ajax-loading-enable":"true","tweak-site-outer-padding":"9","tweak-social-icons-on-right":"true","tweak-user-account-link-position":"With Nav"},"templateId":"56d9c12107eaa07660adbcad","pageFeatures":[1,2,4],"impersonatedSession":false,"demoCollections":[{"collectionId":"57acdb54d2b857e385b27b72","deleted":true},{"collectionId":"57aced14f7e0abc4e9d7cf1c","deleted":true},{"collectionId":"57acf94f725e251f76b9d003","deleted":true},{"collectionId":"57ae1156e3df28c4ce90ea48","deleted":true},{"collectionId":"57ae1164c534a5bee9b19cae","deleted":true},{"collectionId":"57ae119237c58135e955e032","deleted":true},{"collectionId":"57ae11ceff7c50648d8d919c","deleted":true},{"collectionId":"57ae1a57e58c62414bd19b52","deleted":true}],"isFacebookTab":false,"tzData":{"zones":[[-300,"US","E%sT",null]],"rules":{"US":[[1967,2006,null,"Oct","lastSun","2:00","0","S"],[1987,2006,null,"Apr","Sun>=1","2:00","1:00","D"],[2007,"max",null,"Mar","Sun>=8","2:00","1:00","D"],[2007,"max",null,"Nov","Sun>=1","2:00","0","S"]]}},"useNewImageLoader":true};
    script(type="text/javascript").
      SquarespaceFonts.loadViaContext(); Squarespace.load(window);
    script(type="application/ld+json").
      {"url":"https://lingxiaoling.me","name":"Xiao Ling","description":"<p>Personal website. Machine Learning. Computer Vision. Natural language processing. Figure Drawing.</p>","@context":"http://schema.org","@type":"WebSite"}
    //if gte IE 9
      link(rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/59bfff0151a5849dcf7b03ce/8/56d9c12107eaa07660adbcad/59bfff0151a5849dcf7b03e3/339-05142015/1513629132206/site.css?&filterFeatures=false&part=1")
      link(rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/59bfff0151a5849dcf7b03ce/8/56d9c12107eaa07660adbcad/59bfff0151a5849dcf7b03e3/339-05142015/1513629132206/site.css?&filterFeatures=false&part=2")
      link(rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/59bfff0151a5849dcf7b03ce/8/56d9c12107eaa07660adbcad/59bfff0151a5849dcf7b03e3/339-05142015/1513629132206/site.css?&filterFeatures=false&part=3")
      link(rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/59bfff0151a5849dcf7b03ce/8/56d9c12107eaa07660adbcad/59bfff0151a5849dcf7b03e3/339-05142015/1513629132206/site.css?&filterFeatures=false&part=4")
    //if lt IE 9
      link(rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/59bfff0151a5849dcf7b03ce/8/56d9c12107eaa07660adbcad/59bfff0151a5849dcf7b03e3/339-05142015/1513629132206/site.css?&filterFeatures=false&noMedia=true&part=1")
      link(rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/59bfff0151a5849dcf7b03ce/8/56d9c12107eaa07660adbcad/59bfff0151a5849dcf7b03e3/339-05142015/1513629132206/site.css?&filterFeatures=false&noMedia=true&part=2")
      link(rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/59bfff0151a5849dcf7b03ce/8/56d9c12107eaa07660adbcad/59bfff0151a5849dcf7b03e3/339-05142015/1513629132206/site.css?&filterFeatures=false&noMedia=true&part=3")
      link(rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/59bfff0151a5849dcf7b03ce/8/56d9c12107eaa07660adbcad/59bfff0151a5849dcf7b03e3/339-05142015/1513629132206/site.css?&filterFeatures=false&noMedia=true&part=4")
    //if !IE
    link(rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/59bfff0151a5849dcf7b03ce/8/56d9c12107eaa07660adbcad/59bfff0151a5849dcf7b03e3/339-05142015/1513629132206/site.css?&filterFeatures=false")
    // <![endif]
    // End of Squarespace Headers
  body#collection-59c2e758197aea9115b6457b.tweak-social-icons-style-border.tweak-social-icons-shape-circle.tweak-site-ajax-loading-enable.tweak-social-icons-on-right.tweak-cart-link-display-hide.tweak-cart-icon-style-cart.tweak-cart-icon-weight-light.tweak-cart-icon-size-small.tweak-user-account-link-position-with-nav.tweak-user-account-icon-weight-medium.tweak-user-account-icon-size-medium.tweak-show-footer.tweak-nav-style-standard.tweak-nav-position-right.tweak-menu-icon-weight-light.tweak-menu-icon-size-small.tweak-nav-link-style-strikethrough-on-hover.tweak-mobile-nav-alignment-center.tweak-mobile-nav-always-show-tagline.tweak-show-page-title-overlay-never.tweak-page-title-overlay-blend-mode-multiply.tweak-page-title-position-center.tweak-show-page-title.tweak-show-page-desc.tweak-show-page-banner-image-in-index.tweak-index-overlay-blend-mode-multiply.tweak-index-slideshow-touch-on.tweak-index-inactive-on-load.tweak-index-nav-position-bottom-center.tweak-index-nav-layout-inline.tweak-index-menu-link-style-strikethrough-on-hover.tweak-show-index-desc.tweak-index-desc-position-middle.tweak-hide-index-desc-on-hover.tweak-gallery-style-stacked.tweak-full-width-first-landscape.tweak-show-gallery-title-overlay-in-index.tweak-gallery-overlay-blend-mode-normal.tweak-gallery-title-position-center.tweak-show-gallery-title.tweak-show-gallery-desc.tweak-show-gallery-image-captions-always.tweak-gallery-image-caption-indicator-position-bottom-left.tweak-gallery-image-captions-position-center.tweak-gallery-image-captions-below.tweak-gallery-captions-overlay-blend-mode-normal.tweak-blog-meta-primary-category.tweak-blog-meta-secondary-date.tweak-blog-list-style-stacked.tweak-blog-list-separator-show.tweak-blog-list-alignment-center.tweak-blog-list-item-image-aspect-ratio-grid-11-square.tweak-blog-list-item-image-aspect-ratio-stacked-11-square.tweak-blog-list-item-title-show.tweak-blog-list-item-excerpt-show.tweak-blog-list-item-body-show.tweak-blog-list-item-readmore-hide.tweak-blog-list-item-meta-position-below-title.tweak-blog-list-pagination-link-label-show.tweak-blog-list-pagination-link-icon-show.tweak-blog-list-pagination-link-icon-weight-light.tweak-blog-item-alignment-center.tweak-blog-item-meta-position-below-title.tweak-blog-item-share-position-below-content.tweak-blog-item-pagination-link-icon-show.tweak-blog-item-pagination-link-title-show.tweak-blog-item-pagination-link-meta-hide.tweak-blog-item-pagination-link-icon-weight-hairline.event-thumbnails.event-thumbnail-size-32-standard.event-date-label.event-list-show-cats.event-list-date.event-list-time.event-list-address.event-list-icalgcal-links.event-item-back-link.tweak-product-list-image-aspect-ratio-43-four-three.tweak-product-list-item-hover-behavior-fade.tweak-product-list-meta-position-under.tweak-product-list-mobile-meta-position-under.tweak-product-list-meta-alignment-under-center.tweak-product-list-meta-alignment-overlay-center-center.tweak-product-list-show-title.tweak-product-list-show-price.tweak-product-list-filter-display-top.tweak-product-list-filter-alignment-center.tweak-product-item-nav-show-breadcrumb-and-pagination.tweak-product-item-nav-pagination-style-previousnext.tweak-product-item-nav-breadcrumb-alignment-left.tweak-product-item-nav-pagination-alignment-split.tweak-product-item-gallery-position-left.tweak-product-item-gallery-design-stacked.tweak-product-item-gallery-aspect-ratio-11-square.tweak-product-item-gallery-thumbnail-alignment-left.tweak-product-item-details-alignment-left.tweak-product-item-details-show-title.tweak-product-item-details-show-price.tweak-product-item-details-show-excerpt.tweak-product-item-details-excerpt-position-below-price.tweak-product-item-details-show-share-buttons.tweak-product-item-details-show-variants.tweak-product-item-details-show-quantity.tweak-product-item-details-options-style-text-only.tweak-product-item-details-show-add-to-cart-button.tweak-product-item-details-add-to-cart-button-style-outline.tweak-product-item-details-add-to-cart-button-shape-rounded.tweak-product-item-details-add-to-cart-button-padding-small.tweak-product-item-image-zoom-enabled.tweak-product-item-image-zoom-behavior-click.tweak-product-item-lightbox-enabled.tweak-product-badge-style-circle.tweak-product-badge-position-top-right.tweak-product-badge-inset-floating.newsletter-style-dark.hide-opentable-icons.opentable-style-dark.small-button-style-outline.small-button-shape-rounded.medium-button-style-outline.medium-button-shape-rounded.large-button-style-outline.large-button-shape-rounded.image-block-poster-text-alignment-center.image-block-card-dynamic-font-sizing.image-block-card-content-position-center.image-block-card-text-alignment-left.image-block-overlap-dynamic-font-sizing.image-block-overlap-content-position-center.image-block-overlap-text-alignment-left.image-block-collage-dynamic-font-sizing.image-block-collage-content-position-top.image-block-collage-text-alignment-left.image-block-stack-dynamic-font-sizing.image-block-stack-text-alignment-left.button-style-outline.button-corner-style-square.tweak-product-quick-view-button-style-floating.tweak-product-quick-view-button-position-bottom.tweak-product-quick-view-lightbox-excerpt-display-truncate.tweak-product-quick-view-lightbox-show-arrows.tweak-product-quick-view-lightbox-show-close-button.tweak-product-quick-view-lightbox-controls-weight-light.tweak-share-buttons-style-icon-only.tweak-share-buttons-icons-show.tweak-share-buttons-labels-show.tweak-share-buttons-counts-show.tweak-share-buttons-standard-background-color.native-currency-code-usd.collection-59c2e758197aea9115b6457b.collection-type-page.collection-layout-default.mobile-style-available.has-index-nav(data-controller="MainContentPositioning, MercuryLoader")
    .mercury-transition-wrapper
      .content-outer-wrapper(data-controller="HeaderOverflow, NavToggle, HeaderScroll, OverlayNavOverflow, SocialIconFadein, SetActiveNavLink")
        .overlay-nav-wrapper
          .overlay-nav-inner-wrapper
            nav.main-navigation--overlay
              .nav-item.external
                a(href="/s/cv-he7k.pdf")
                  span CV
        header#siteHeader.site-header
          .site-branding(data-content-field="site-title")
            h1.site-title
              a(href="/")
                span.site-title Xiao Ling
          a#navToggle.nav-toggle.flex-item
            div
              svg.Icon.Icon--hamburger.nav-toggle-label-icon(viewbox="0 0 24 18")
                g.svg-icon.use--even
                  line(fill="none" stroke-miterlimit="10" x1="0" y1="2" x2="24" y2="2")
                  line(fill="none" stroke-miterlimit="10" x1="0" y1="9" x2="24" y2="9")
                  line(fill="none" stroke-miterlimit="10" x1="0" y1="16" x2="24" y2="16")
                g.svg-icon.use--odd
                  line(fill="none" stroke-miterlimit="10" x1="0" y1="1.5" x2="24" y2="1.5")
                  line(fill="none" stroke-miterlimit="10" x1="0" y1="8.5" x2="24" y2="8.5")
                  line(fill="none" stroke-miterlimit="10" x1="0" y1="15.5" x2="24" y2="15.5")
              svg.Icon.Icon--close.nav-toggle-label-icon(viewbox="0 0 18 18")
                g.svg-icon
                  line(fill="none" stroke-miterlimit="10" stroke-linecap="butt" x1="0" y1="0" x2="18" y2="18")
                  line(fill="none" stroke-miterlimit="10" stroke-linecap="butt" x1="18" y1="0" x2="0" y2="18")
          .nav-social-wrapper(data-content-field="navigation" data-annotation-alignment="bottom")
            nav#mainNavigation.main-navigation.has-items
              .nav-item.external
                a(href="/s/cv-he7k.pdf")
                  span CV
        #mainContent.main-content(role="main")
          .page-banner-wrapper.no-main-image(data-edit-main-image="Background" data-collection-id="59c2e758197aea9115b6457b" data-controller="PageBanners")
          .title-card-wrapper(data-controller="TitleCardHandler")
            .overlay.blended-overlay.page-title-overlay.title-card-overlay
            .title-card(data-slide-id="title-card")
              .title-desc-outer-wrapper
                .title-desc-inner-wrapper
                  h1.page-title
                    span(data-content-field="title")
                      | Deep Reinforcement Learning with Hierarchical Recurrent Encoder-Decoder for Conversation
                  .page-desc(data-content-field="description")
          .main-content-inner-wrapper(data-content-field="main-content")
            #page-59c2e758197aea9115b6457b.sqs-layout.sqs-grid-12.columns-12(data-type="page" data-updated-on="1513616943309")
              .row.sqs-row
                .col.sqs-col-12.span-12
                  #block-31bc36482659076d2784.sqs-block.html-block.sqs-block-html(data-block-type="2")
                    .sqs-block-content
                      h1.text-align-center
                        strong Deep Reinforcement Learning with 
                        strong
                          | Hierarchical
                          br
                          | Recurrent Encoder-Decoder for Conversation
                  #block-a56be9e646b3401dd85e.sqs-block.image-block.sqs-block-image(data-aspect-ratio="60" data-block-type="5")
                    .sqs-block-content
                      .image-block-outer-wrapper.layout-caption-below.design-layout-inline
                        .intrinsic(style="max-width:1600.0px;")
                          .image-block-wrapper.has-aspect-ratio(style="padding-bottom:60.0%;" data-description="")
                            noscript
                              img(src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a21590ae4966be2ad884a0f/1512134926475/G-home-1.jpeg" alt="G-home-1.jpeg")
                            img.thumb-image(alt="G-home-1.jpeg" data-src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a21590ae4966be2ad884a0f/1512134926475/G-home-1.jpeg" data-image="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a21590ae4966be2ad884a0f/1512134926475/G-home-1.jpeg" data-image-dimensions="1600x900" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a21590ae4966be2ad884a0f" data-type="image")
                  #block-32fe810acc8721a94269.sqs-block.spacer-block.sqs-block-spacer.sized.vsize-1(data-block-type="21")
                    .sqs-block-content  
              .row.sqs-row
                .col.sqs-col-1.span-1
                  #block-377a51a1257979423374.sqs-block.spacer-block.sqs-block-spacer.sized.vsize-1(data-block-type="21")
                    .sqs-block-content  
                .col.sqs-col-10.span-10
                  #block-17f321948bf8eedab8d9.sqs-block.html-block.sqs-block-html(data-block-type="2")
                    .sqs-block-content
                      h2 ABSTRACT
                  #block-yui_3_17_2_5_1512316064481_75368.sqs-block.image-block.sqs-block-image.sqs-col-5.span-5.float.float-right(data-block-type="5")
                    .sqs-block-content
                      .image-block-outer-wrapper.layout-caption-below.design-layout-inline
                        .intrinsic(style="max-width:829.0px;")
                          .image-block-wrapper.has-aspect-ratio(style="padding-bottom:52.95536422729492%;" data-description="")
                            noscript
                              img(src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a242dc824a694ef80492444/1512320460680/echo-2.jpg" alt="echo-2.jpg")
                            img.thumb-image(alt="echo-2.jpg" data-src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a242dc824a694ef80492444/1512320460680/echo-2.jpg" data-image="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a242dc824a694ef80492444/1512320460680/echo-2.jpg" data-image-dimensions="829x439" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a242dc824a694ef80492444" data-type="image")
                  #block-yui_3_17_2_5_1512316064481_80478.sqs-block.html-block.sqs-block-html(data-block-type="2")
                    .sqs-block-content
                      p
                        | I co-led the project with 
                        a(target="_blank" href="https://www.chloehjeong.com/") Heejin Jeong
                        |  and built an intelligent dialogue system that was deployed on the Echo Dot. It is well known that dialogue systems relying on hand-crafted rules are too inflexible to handle the myriad of sentences in a natural conversation. And although current recurrent neural net based systems are more robust to input, they have a tendency to generate irrelevant or curt responses that terminates a conversation prematurely. We hypothesized that an engaging dialogue system should satisfy two criteria: it must incorporate the dialogue history when deciding what to say next, and it must be able to understand the future consequence of its utterances. We posed the “history aware” problem as an optimization problem with multiple objectives where by both likelihood of the current sentence given previous sentences, and the likelihood of the current word given previous words are maximized. This complex distribution is represented by a hierarchical neural net. We posed the “future aware” requirement as a deep reinforcement learning problem, whereby the dialogue agent optimized a conversation policy given specified future reward.  Our model was trained on CALLHOME American English speech corpus as well as the OpenSubtitles corpus.  The 
                        a(target="_blank" href="https://github.com/lingxiao/neural-chatbot/blob/master/paper/final_hj.pdf") paper
                        |  was submitted to The Mid-Atlantic Student Colloquium on Speech, Language and Learning and accepted for poster presentation.
                      h2 INTRODUCTION
                  #block-yui_3_17_2_2_1512075837160_19774.sqs-block.image-block.sqs-block-image.sqs-col-4.span-4.float.float-left(data-block-type="5")
                    .sqs-block-content
                      .image-block-outer-wrapper.layout-caption-below.design-layout-inline
                        .intrinsic(style="max-width:2197.0px;")
                          .image-block-wrapper.has-aspect-ratio(style="padding-bottom:66.49977111816406%;" data-description="")
                            noscript
                              img(src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a20732d24a694eb0d092fe9/1512076201663/turing-1.jpg" alt="turing-1.jpg")
                            img.thumb-image(alt="turing-1.jpg" data-src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a20732d24a694eb0d092fe9/1512076201663/turing-1.jpg" data-image="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a20732d24a694eb0d092fe9/1512076201663/turing-1.jpg" data-image-dimensions="2197x1461" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a20732d24a694eb0d092fe9" data-type="image")
                  #block-yui_3_17_2_2_1512075837160_27649.sqs-block.html-block.sqs-block-html(data-block-type="2")
                    .sqs-block-content
                      blockquote(style="margin-left:80px")
                        em
                          | “A computer would deserve to be called intelligent if it could deceive a human into believing that it was human"
                          br
                          |             - Alan Turing
                      p
                        | A long standing goal of artificial intelligence has been to program a computer that can carry on a conversation intelligently with a human being.  The first milestone in this direction was ELIZA, created in 1964 at MIT Artificial Intelligence Laboratory by Dr. Joseph Weizenbaum. ELIZA simulated “the responses of a  non-directional psychotherapist in an initial psychiatric interview,” a shrink asking open-ended questions. Some of its responses were so convincing that users became emotionally attached to the program. This led many academics at the time to believe that ELIZA could positively impact people’s mental well being.  However, its rule based system was ultimately too restrictive for prolonged conversation, and this line of research was discontinued after the AI winter.
                      p
                        span(style="letter-spacing:0.01em")
                          | In recent years, the proliferation of home assistant devices such as the Alexa and Google Home led to a resurgence of interest in intelligent conversation agents, so called “chatbots.”  Broadly speaking, chabots are expected to carry on two kinds of conversation: informational-retrieval based conversation where the user expects the correct answer for a specific query; and open-domain dialog, where the chatbot is expected to speak about any topic and be “interesting.”  We focus on the second task, using the latest machine learning techniques to train an algorithm on conversation etiquette using transcripts from movie scripts and recorded phone calls.
                      h2
                        br
                        span(style="font-size:22px") TECHNICAL BACKGROUND
                      p
                        | The next two sections describes the technical background required to understand this work. We assume a basic understanding of computer science, probability, and linear algebra.  And reasonable familiarity with machine learning both in concept and practice.
                      h3 Recurrent neural network
                  #block-yui_3_17_2_4_1512338267045_10871.sqs-block.image-block.sqs-block-image.sqs-col-5.span-5.float.float-right(data-block-type="5")
                    .sqs-block-content
                      .image-block-outer-wrapper.layout-caption-below.design-layout-inline
                        .intrinsic(style="max-width:275.0px;")
                          .image-block-wrapper.has-aspect-ratio(style="padding-bottom:153.4545440673828%;" data-description='<p>Fig. 1. Cartoon of a simple Recurrent Neural Net. Diagram from <a target="_blank" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah</a>.</p>')
                            noscript
                              img(src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a24738be2c483bcf3dc32aa/1512404964157/RNN-1.png" alt=" Fig. 1. Cartoon of a simple Recurrent Neural Net. Diagram from  Colah . ")
                            img.thumb-image(alt=" Fig. 1. Cartoon of a simple Recurrent Neural Net. Diagram from  Colah . " data-src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a24738be2c483bcf3dc32aa/1512404964157/RNN-1.png" data-image="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a24738be2c483bcf3dc32aa/1512404964157/RNN-1.png" data-image-dimensions="275x422" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a24738be2c483bcf3dc32aa" data-type="image")
                          .image-caption-wrapper
                            .image-caption
                              p
                                | Fig. 1. Cartoon of a simple Recurrent Neural Net. Diagram from 
                                a(target="_blank" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/") Colah
                                | .
                  #block-yui_3_17_2_4_1512338267045_13502.sqs-block.html-block.sqs-block-html(data-block-type="2")
                    .sqs-block-content
                      p
                        | We only provide a brief description of recurrent neural nets (RNNs) here. A great introduction to RNN and its popular extension the long short term memory networks (LSTM) can be found 
                        a(target="_blank" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/") here
                        | . RNNs are a family of networks for processing sequences of values, i.e., sentences. For example in Fig 1, the RNN denoted by A will take in some word x in the English language, and output a “hidden” state h. Notably, the arrow that stems from A and loops back onto the box symbolizes the fact that the hidden output is a both a function of the input x, as well as the previous hidden state stored in the network. The representative power of RNNs stems from the fact it has short term memory spanning several symbols in the sequence, which is essential in natural language sentences that have long term dependencies. In practice, the memory capacity of RNNs proved insufficient to model real languages, thus the LSTMs architecture was introduced to explicitly model long term dependencies.
                  #block-yui_3_17_2_4_1512338267045_60000.sqs-block.image-block.sqs-block-image.sqs-col-5.span-5.float.float-left(data-block-type="5")
                    .sqs-block-content
                      .image-block-outer-wrapper.layout-caption-below.design-layout-inline
                        .intrinsic(style="max-width:605.0px;")
                          .image-block-wrapper.has-aspect-ratio(style="padding-bottom:59.338844299316406%;" data-description='<p>Fig 2. A projection of hidden representations onto two dimensions. Note semantically similar words have smaller cosine distance.&nbsp;From <a target="_blank" href="http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf">Turian <em>et al.</em>&nbsp;(2010)</a>.&nbsp;</p>')
                            noscript
                              img(src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a24798bf9619ac01b0dccd7/1512339854007/embedding-1.png" alt=" Fig 2. A projection of hidden representations onto two dimensions. Note semantically similar words have smaller cosine distance.&nbsp;From  Turian  et al. &nbsp;(2010) .&nbsp; ")
                            img.thumb-image(alt=" Fig 2. A projection of hidden representations onto two dimensions. Note semantically similar words have smaller cosine distance.&nbsp;From  Turian  et al. &nbsp;(2010) .&nbsp; " data-src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a24798bf9619ac01b0dccd7/1512339854007/embedding-1.png" data-image="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a24798bf9619ac01b0dccd7/1512339854007/embedding-1.png" data-image-dimensions="605x359" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a24798bf9619ac01b0dccd7" data-type="image")
                          .image-caption-wrapper
                            .image-caption
                              p
                                | Fig 2. A projection of hidden representations onto two dimensions. Note semantically similar words have smaller cosine distance. From 
                                a(target="_blank" href="http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf")
                                  | Turian 
                                  em et al.
                                  |  (2010)
                                | .
                  #block-yui_3_17_2_4_1512338267045_62237.sqs-block.html-block.sqs-block-html(data-block-type="2")
                    .sqs-block-content
                      p
                        | Finally, let us examine a specific RNN model and why it matters. In 
                        a(href="https://en.wikipedia.org/wiki/Continuous_bag-of-words") continuous bag-of-words
                        |  model, the RNN is given news articles and must predict the current word given a window of surrounding context words. In this case, the input x is a one-hot representation while the output h is some n-dimensional vector. The power of model is evident when examining the hidden representations, semantically similar words are closer together under under cosine distance than those with dissimilar meanings. In a very rough sense, continuous bag-of-words relate meaning of words to geometry, also known as an embedding. There are numerous such models, collectively called word2vec. Although word2vec has performed admirably on certain word analogy tasks, there is no unique embedding that captures all the various relationships between words at the same time, ie antonyms, hypernyms, meronyms. In fact for many of these relationships, there is tremendous merit to 
                        a(target="_blank" href="https://lingxiaoling.me/new-page-3") explicitly learning them from data
                        | . 
                        span(style="letter-spacing:0.01em")
                          | Word2vec models certainly do not relate words to the myriad of entities they may symbolize.  Therefore word2vec do not solve the
                        a(target="_blank" style="letter-spacing: 0.01em;" href="https://en.wikipedia.org/wiki/Symbol_grounding_problem")
                          strong  symbol grounding problem
                        span(style="letter-spacing:0.01em") . 
                      h3 Reinforcement Learning
                  #block-yui_3_17_2_2_1512340736618_82341.sqs-block.code-block.sqs-block-code.sqs-col-5.span-5.float.float-right(data-block-type="23")
                    .sqs-block-content
                      img(src="https://i.makeagif.com/media/7-26-2015/Xf4zu7.gif" style="" -="" 0;="" margin:="")
                      h3(style="font-size: 10px") Fig 3. Pew Pew. 
                  #block-yui_3_17_2_2_1512340736618_82408.sqs-block.html-block.sqs-block-html(data-block-type="2")
                    .sqs-block-content
                      p
                        | This section gives a gentle introduction to reinforcement learning (RL) and deep RL. The essence of RL is learning through interaction. An RL agent interacts with its environment and, upon observing the consequences of its actions, can learn to alter its own behaviour in response to rewards received. This paradigm of trial and error learning is rooted in psychology. Specifically, the autonomous agent has a set of actions A it can take on the environment, there is a set of states S, and a set of rewards R that the agent may receive upon each action. The agent will take an action a at each state s, and upon transitioning to a new state, receive a reward r from the environment.  The agent’s goal is to determine a policy of which action to take in each state so that its reward is maximized at the end of the game. The simplest way to represent policy is as a table mapping states and actions to rewards. Historically, the states and actions of the game is constructed by hand, this is sufficient for simple games (such as tic-tac-toe) but do not scale well to games with high dimensional input and no clear heuristic as to how to represent the states. In the game Space Invaders (Fig 3) for example, the states could be every possible image frame over all possible space invaders games, and the action are all possible left, right, and shoot combinations the player could take. Therefore both the state space and the policy is too large so as to render the problem intractable. Deep RL promises to solve the intractability problem, for example a deep convolutional neural net is first used to compress the states in Space Invaders, then RL is used to approximate a policy with this reduced state space.
                  #block-yui_3_17_2_2_1512340736618_62585.sqs-block.image-block.sqs-block-image(data-block-type="5")
                    .sqs-block-content
                      .image-block-outer-wrapper.layout-caption-below.design-layout-inline
                        .intrinsic(style="max-width:2229.0px;")
                          .image-block-wrapper.has-aspect-ratio(style="padding-bottom:16.958276748657227%;" data-description='<p>Fig 4. First, a convolutional neural net is used to compress the states of Space Invaders, then RL is used to learn an optimal policy of the game over the approximated state space.&nbsp; From <a target="_blank" href="https://arxiv.org/pdf/1708.05866.pdf">Arulkumaran et al. (2017)</a></p>')
                            noscript
                              img(src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a24b1a88165f506770e987b/1512354221067/deep-RL-1.png" alt=" Fig 4. First, a convolutional neural net is used to compress the states of Space Invaders, then RL is used to learn an optimal policy of the game over the approximated state space.&nbsp; From  Arulkumaran et al. (2017)  ")
                            img.thumb-image(alt=" Fig 4. First, a convolutional neural net is used to compress the states of Space Invaders, then RL is used to learn an optimal policy of the game over the approximated state space.&nbsp; From  Arulkumaran et al. (2017)  " data-src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a24b1a88165f506770e987b/1512354221067/deep-RL-1.png" data-image="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a24b1a88165f506770e987b/1512354221067/deep-RL-1.png" data-image-dimensions="2229x378" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a24b1a88165f506770e987b" data-type="image")
                          .image-caption-wrapper
                            .image-caption
                              p
                                | Fig 4. First, a convolutional neural net is used to compress the states of Space Invaders, then RL is used to learn an optimal policy of the game over the approximated state space.  From 
                                a(target="_blank" href="https://arxiv.org/pdf/1708.05866.pdf") Arulkumaran et al. (2017)
                  #block-yui_3_17_2_2_1512340736618_62863.sqs-block.html-block.sqs-block-html(data-block-type="2")
                    .sqs-block-content
                      h2 PROBLEM FORMULATION
                      p
                        | This section describes how we formulated the problem of training an intelligent dialogue agent using LSTMs and Deep RL. We model the conversation as a sequence of utterances between two speakers, each utterance is composed of a sequence of words. The goal of the intelligent agent is then to output the “best” response given 
                        em all 
                        | previous utterances, and the expected reward of outputting this response.
                  #block-yui_3_17_2_4_1512397748778_78137.sqs-block.image-block.sqs-block-image.sqs-col-5.span-5.float.float-right(data-block-type="5")
                    .sqs-block-content
                      .image-block-outer-wrapper.layout-caption-below.design-layout-inline
                        .intrinsic(style="max-width:539.0px;")
                          .image-block-wrapper.has-aspect-ratio(style="padding-bottom:29.31354331970215%;" data-description='<p>Fig 5. As a baseline, we use seq2seq framework to model conversation. The token &lt;eos&gt; denotes the end-of-sentence, a special symbol inserted into the end of each sentence during preprocessing. From <strong><a target="_blank" href="https://arxiv.org/pdf/1506.05869.pdf">Vinyals and Le</a>&nbsp;(2015).</strong></p>')
                            noscript
                              img(src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a25720d0d92971cbf028ffb/1512403478931/seq2seq-1.png" alt=" Fig 5. As a baseline, we use seq2seq framework to model conversation. The token &lt;eos&gt; denotes the end-of-sentence, a special symbol inserted into the end of each sentence during preprocessing. From   Vinyals and Le &nbsp;(2015).  ")
                            img.thumb-image(alt=" Fig 5. As a baseline, we use seq2seq framework to model conversation. The token &lt;eos&gt; denotes the end-of-sentence, a special symbol inserted into the end of each sentence during preprocessing. From   Vinyals and Le &nbsp;(2015).  " data-src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a25720d0d92971cbf028ffb/1512403478931/seq2seq-1.png" data-image="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a25720d0d92971cbf028ffb/1512403478931/seq2seq-1.png" data-image-dimensions="539x158" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a25720d0d92971cbf028ffb" data-type="image")
                          .image-caption-wrapper
                            .image-caption
                              p
                                | Fig 5. As a baseline, we use seq2seq framework to model conversation. The token <eos> denotes the end-of-sentence, a special symbol inserted into the end of each sentence during preprocessing. From 
                                strong
                                  a(target="_blank" href="https://arxiv.org/pdf/1506.05869.pdf") Vinyals and Le
                                  |  (2015).
                  #block-yui_3_17_2_4_1512397748778_174487.sqs-block.html-block.sqs-block-html(data-block-type="2")
                    .sqs-block-content
                      p
                        | As a baseline, we replicated the work of 
                        a(target="_blank" href="https://arxiv.org/pdf/1506.05869.pdf") Vinyals and Le
                        |  (2015). They cast the best response given previous response as a sequence-to-sequence (seq2seq) learning problem. In seq2seq, an encoder LSTM reads the input sequence on token at a time, and keeps track of a hidden state that is updated upon each word read. This hidden state may be loosely interpreted as a “summary” of the sentence, sometimes referred to as an utterance vector. The summary vector is then passed to a decoder LSTM, which predicts the output sequence one word at a time, while updating the same utterance vector from the encoder LSTM. During training, the true output sequence is given to the decoder LSTM, and the model is trained to maximize the cross entropy of the correct sequence given its context.
                  #block-yui_3_17_2_4_1512421368576_130126.sqs-block.image-block.sqs-block-image.sqs-col-5.span-5.float.float-left(data-block-type="5")
                    .sqs-block-content
                      .image-block-outer-wrapper.layout-caption-below.design-layout-inline
                        .intrinsic(style="max-width:600.0px;")
                          .image-block-wrapper.has-aspect-ratio(style="padding-bottom:102.16666412353516%;" data-description="<p>Fig 6. The conversation is modeled as a random walk through discourse space.</p>")
                            noscript
                              img(src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a25bf359140b79633c3f44a/1512423224610/randwalk-2.jpg" alt=" Fig 6. The conversation is modeled as a random walk through discourse space. ")
                            img.thumb-image(alt=" Fig 6. The conversation is modeled as a random walk through discourse space. " data-src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a25bf359140b79633c3f44a/1512423224610/randwalk-2.jpg" data-image="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a25bf359140b79633c3f44a/1512423224610/randwalk-2.jpg" data-image-dimensions="600x613" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a25bf359140b79633c3f44a" data-type="image")
                          .image-caption-wrapper
                            .image-caption
                              p Fig 6. The conversation is modeled as a random walk through discourse space.
                  #block-yui_3_17_2_4_1512421368576_130403.sqs-block.html-block.sqs-block-html(data-block-type="2")
                    .sqs-block-content
                      p
                        | Next, we tested the hypothesis that the dialogue agent benefits from a richer representation of the conversation history. Specifically, we model the conversation as a 
                        a(target="_blank" href="https://arxiv.org/pdf/1502.03520.pdf") random walk over a “discourse space”
                        | , where each vector in this space represent some abstract topic of the conversation. Then at each time point, the discourse vector is projected onto the utterance space, and a sequence of words is generated conditioned on this projected discourse vector. We represent this process using a hierarchical recurrent encoder-decoder architecture (HRED), first proposed by 
                        a(target="_blank" href="https://arxiv.org/pdf/1507.02221.pdf") Sordoni et al
                        | . (2015a) for query prediction, and later adapted by 
                        a(target="_blank" href="https://arxiv.org/pdf/1507.04808.pdf") Serban et al
                        | . (2016) for neural dialogue generation. Under this model, and encoder LSTM maps each utterance to an utterance vector.   Then an higher-level context LSTM maps this utterance vector to the discourse vector, and propagates this discourse vector between conversations. The decoder LSTM is then used to generate the response conditioned on the discourse vector. Finally, both the baseline seq2seq model and HRED are trained by maximizing the log likelihood (MLE) of the entire session.
                  #block-yui_3_17_2_4_1512421368576_18304.sqs-block.image-block.sqs-block-image(data-block-type="5")
                    .sqs-block-content
                      .image-block-outer-wrapper.layout-caption-below.design-layout-inline
                        .intrinsic(style="max-width:700.0px;")
                          .image-block-wrapper.has-aspect-ratio(style="padding-bottom:51.28571319580078%;" data-description='<p>Fig 7. HRED architecture. Each utterance is encoded into an "utterance representation" and mapped into the hidden discourse vector, which is used to generate the words in the response. This hierarchical structure allows information to propagate farther along the conversation, thus maintaining the conversation history.&nbsp; From <a target="_blank" href="https://arxiv.org/pdf/1507.04808.pdf">Serban et al</a>. (2016).</p>')
                            noscript
                              img(src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a25b863085229ae8ce3972d/1512421480345/hred-2.png" alt=' Fig 7. HRED architecture. Each utterance is encoded into an "utterance representation" and mapped into the hidden discourse vector, which is used to generate the words in the response. This hierarchical structure allows information to propagate farther along the conversation, thus maintaining the conversation history.&nbsp; From  Serban et al . (2016). ')
                            img.thumb-image(alt=' Fig 7. HRED architecture. Each utterance is encoded into an "utterance representation" and mapped into the hidden discourse vector, which is used to generate the words in the response. This hierarchical structure allows information to propagate farther along the conversation, thus maintaining the conversation history.&nbsp; From  Serban et al . (2016). ' data-src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a25b863085229ae8ce3972d/1512421480345/hred-2.png" data-image="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a25b863085229ae8ce3972d/1512421480345/hred-2.png" data-image-dimensions="700x359" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a25b863085229ae8ce3972d" data-type="image")
                          .image-caption-wrapper
                            .image-caption
                              p
                                | Fig 7. HRED architecture. Each utterance is encoded into an "utterance representation" and mapped into the hidden discourse vector, which is used to generate the words in the response. This hierarchical structure allows information to propagate farther along the conversation, thus maintaining the conversation history.  From 
                                a(target="_blank" href="https://arxiv.org/pdf/1507.04808.pdf") Serban et al
                                | . (2016).
                  #block-yui_3_17_2_4_1512421368576_266563.sqs-block.image-block.sqs-block-image.sqs-col-5.span-5.float.float-right(data-block-type="5")
                    .sqs-block-content
                      .image-block-outer-wrapper.layout-caption-below.design-layout-inline
                        .intrinsic(style="max-width:2109.0px;")
                          .image-block-wrapper.has-aspect-ratio(style="padding-bottom:53.247982025146484%;" data-description='<p>Fig 8. Similar to Samantha from the film<a target="_blank" href="https://www.youtube.com/watch?v=23JrtlxZxxk"> Her (Spike Jonze, 2013)</a>, our neural dialogue agents learns conversation etiquette over the course of many dialogue sessions, in this case with itself.</p>')
                            noscript
                              img(src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a25c53fc8302554b580453a/1512442307747/" alt=" Fig 8. Similar to Samantha from the film  Her (Spike Jonze, 2013) , our neural dialogue agents learns conversation etiquette over the course of many dialogue sessions, in this case with itself. ")
                            img.thumb-image(alt=" Fig 8. Similar to Samantha from the film  Her (Spike Jonze, 2013) , our neural dialogue agents learns conversation etiquette over the course of many dialogue sessions, in this case with itself. " data-src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a25c53fc8302554b580453a/1512442307747/" data-image="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a25c53fc8302554b580453a/1512442307747/" data-image-dimensions="2109x1123" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a25c53fc8302554b580453a" data-type="image")
                          .image-caption-wrapper
                            .image-caption
                              p
                                | Fig 8. Similar to Samantha from the film
                                a(target="_blank" href="https://www.youtube.com/watch?v=23JrtlxZxxk")  Her (Spike Jonze, 2013)
                                | , our neural dialogue agents learns conversation etiquette over the course of many dialogue sessions, in this case with itself.
                  #block-yui_3_17_2_4_1512421368576_60274.sqs-block.html-block.sqs-block-html(data-block-type="2")
                    .sqs-block-content
                      p
                        | Baseline models trained using MLE objectives often output short-sighted and dull responses such as “I don’t know.”  This is not surprising since the MLE objective does not capture the original intent of developing an open-domain dialogue agent that engages the user. We remedy this problem using deep reinforcement learning with hand-crafted reward functions. In our setting, the actions of the agent are the responses it can output, the states are defined by the previous two turns of the dialogue, while the policy is represented by the two baseline encoder-decoder networks we trained using the MLE objective. Following the work of
                        a(target="_blank" href="https://aclweb.org/anthology/D16-1127")  Li et al
                        | . (2016), our reward function is a weighted sum of three hand crafted objectives that promote “engaging” dialogue behavior, and the weights are tuned heuristically during training time. The first objective penalizes “dull” responses, defined as eight or more consecutive responses of key phrase such as “I don’t know”, “I have no idea”, and “I don’t know what you’re talking about”, etc. The second function penalizes semantic similarly defined by the cosine distance of the utterance vector of the query and response, this prevents responses that simply paraphrase the user query. The last reward promotes semantic coherence, defined by the probability of the response given the previous state. When searching for the optimal policy using RL, we first initialized the policy using the encoder-decoder trained using the MLE objective, so that it already outputs plausible responses. Then we simulated a conversation between two dialogue agents so that they may find the optimal conversation policy.
                      h2 EXPERIMENT
                      h3 DATASET
                  #block-yui_3_17_2_4_1512487254930_43846.sqs-block.image-block.sqs-block-image.sqs-col-5.span-5.float.float-left(data-block-type="5")
                    .sqs-block-content
                      .image-block-outer-wrapper.layout-caption-below.design-layout-inline
                        .intrinsic(style="max-width:1393.0px;")
                          .image-block-wrapper.has-aspect-ratio(style="padding-bottom:51.90237045288086%;" data-description="<p>Fig 9.&nbsp; CALLHOME corpus. Note it is very common for very long queries to be followed by short responses&nbsp;</p>")
                            noscript
                              img(src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a26c8ea652deaad92cef608/1512491246316/callhome.png" alt=" Fig 9.&nbsp; CALLHOME corpus. Note it is very common for very long queries to be followed by short responses&nbsp; ")
                            img.thumb-image(alt=" Fig 9.&nbsp; CALLHOME corpus. Note it is very common for very long queries to be followed by short responses&nbsp; " data-src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a26c8ea652deaad92cef608/1512491246316/callhome.png" data-image="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a26c8ea652deaad92cef608/1512491246316/callhome.png" data-image-dimensions="1393x723" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a26c8ea652deaad92cef608" data-type="image")
                          .image-caption-wrapper
                            .image-caption
                              p
                                | Fig 9.  CALLHOME corpus. Note it is very common for very long queries to be followed by short responses
                  #block-yui_3_17_2_4_1512487254930_56989.sqs-block.image-block.sqs-block-image.sqs-col-5.span-5.float.float-left(data-block-type="5")
                    .sqs-block-content
                      .image-block-outer-wrapper.layout-caption-below.design-layout-inline
                        .intrinsic(style="max-width:1465.0px;")
                          .image-block-wrapper.has-aspect-ratio(style="padding-bottom:36.99658966064453%;" data-description="<p>Fig 10. OpenSubtitles Corpus. Since we do not have access to the original movie script, we heuristically assigned utterances to speakers.&nbsp;</p>")
                            noscript
                              img(src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a26c923f9619ab1f724fe7e/1512491300660/" alt=" Fig 10. OpenSubtitles Corpus. Since we do not have access to the original movie script, we heuristically assigned utterances to speakers.&nbsp; ")
                            img.thumb-image(alt=" Fig 10. OpenSubtitles Corpus. Since we do not have access to the original movie script, we heuristically assigned utterances to speakers.&nbsp; " data-src="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a26c923f9619ab1f724fe7e/1512491300660/" data-image="https://static1.squarespace.com/static/59bfff0151a5849dcf7b03ce/t/5a26c923f9619ab1f724fe7e/1512491300660/" data-image-dimensions="1465x542" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a26c923f9619ab1f724fe7e" data-type="image")
                          .image-caption-wrapper
                            .image-caption
                              p
                                | Fig 10. OpenSubtitles Corpus. Since we do not have access to the original movie script, we heuristically assigned utterances to speakers.
                  #block-yui_3_17_2_4_1512487254930_215855.sqs-block.html-block.sqs-block-html(data-block-type="2")
                    .sqs-block-content
                      p
                        | The baseline models are trained on two data sets. The first one is CALLHOME American English Speech corpus (
                        a(target="_blank" href="https://catalog.ldc.upenn.edu/ldc97s42") LDC97S42
                        | ), consisting of 120 30-minute phone conversations.  We also used from the 
                        a(target="_blank" href="http://opus.nlpl.eu/") OpenSubtitles
                        |  corpus. Both corpus were normalized by case-folding, whitespace stripping, and converting all tokens to lower cases. Proper nouns and numbers were kept as is. The quality of data is extremely important in machine learning, and there were significant weaknesses in both corpus. In the CALLHOME corpus, we saw it was very common for short responses such as “uh-uh” to follow very long queries (see figure 9). This biases the algorithm to output short responses when it encounters a very long sentence. In the OpenSubtitles corpus the speakers were not annotated, and there were multiple speakers. Thus we heuristically assigned each turn to one of two speakers, clearly this is far from ideal. These challenges underscore the weaknesses of data-driven dialogue generation: it is very difficult to acquire natural dialogues from daily life due to privacy constraints.
                      h3
                        strong experimental results
                      p Experiments for this projects are ongoing! 
                      h2 CONCLUSION
                      h2 TEAM
                      p
                        a(target="_blank" href="https://www.chloehjeong.com/") Heejin Jeong
                        | , Xiao Ling
                      h2 DETAILS 
                      p
                        a(target="_blank" href="https://github.com/lingxiao/neural-chatbot/blob/master/paper/final_hj.pdf") paper
                        | , 
                        a(target="_blank" href="https://github.com/lingxiao/neural-chatbot")  source code
                      p  
                  #block-4f3bcdfbee05ff2784a3.sqs-block.spacer-block.sqs-block-spacer.sized.vsize-1(data-block-type="21")
                    .sqs-block-content  
                .col.sqs-col-1.span-1
                  #block-db409702d9e6ffa3f593.sqs-block.spacer-block.sqs-block-spacer.sized.vsize-1(data-block-type="21")
                    .sqs-block-content  
          footer.site-footer
            #footer-blocks.sqs-layout.sqs-grid-12.columns-12.empty(data-layout-label="Footer Content" data-type="block-field" data-updated-on="1505944866943")
              .row.sqs-row
                .col.sqs-col-12.span-12
    .index-nav
      .index-gallery-wrapper.index-item-navigation(data-controller="IndexSetup, IndexPositioning, IndexNavOverflow, IndexNavScroll")
        .collection-images.index-nav-images
          .image-container.no-main-image-bg.active.active-page(data-url-id="drl-dialogue")
          .image-container.no-main-image-bg(data-url-id="scalar-adj")
          .image-container.no-main-image-bg(data-url-id="new-page-2")
          .image-container.no-main-image-bg(data-url-id="new-page")
          .overlay.blended-overlay.index-overlay
        .collection-nav-desc-wrapper
          nav.index-navigation.collection-nav(role="navigation")
            .collection-nav-item.active.active-page(data-url-id="drl-dialogue")
              a.drl-dialogue(href="/drl-dialogue/")
                span.collection-nav-item-span
                  | Deep Reinforcement Learning with Hierarchical Recurrent Encoder-Decoder for Conversation
            .collection-nav-item(data-url-id="scalar-adj")
              a.scalar-adj(href="/scalar-adj/")
                span.collection-nav-item-span Using Paraphrases to Cluster and Order Adjectives by Intensity
            .collection-nav-item(data-url-id="new-page-2")
              a.new-page-2(href="/new-page-2/")
                span.collection-nav-item-span Multilingual Corpus for Learning Translations from Images
            .collection-nav-item(data-url-id="new-page")
              a.new-page(href="/new-page/")
                span.collection-nav-item-span Learning Translations via Matrix Completion
    script(src="https://static1.squarespace.com/static/ta/56d9c0fe40261d18462df72d/339/scripts/site-bundle.js" type="text/javascript")
    script(type="text/javascript" data-sqs-type="imageloader-bootstraper").
      (function() {if(window.ImageLoader) { window.ImageLoader.bootstrap({}, document); }})();
    script.
      Squarespace.afterBodyLoad(Y);
